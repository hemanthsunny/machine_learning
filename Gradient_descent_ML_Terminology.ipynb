{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gradient descent / ML Terminology",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNZOPAlRoYDVgIACLxPYLYL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hemanthsunny/machine_learning/blob/master/Gradient_descent_ML_Terminology.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BesEx-J2aoEA",
        "colab_type": "text"
      },
      "source": [
        "*Stochastic gradient descent* = Iterative learning algorithm / optimization algorithm - has many hyperparameters <br>\n",
        "2 hyperparams - batch size, number of epochs - Both integer values & do the same thing <br>\n",
        "1. **Batch size** = a hyperparam of gradient descent - controls the number of training samples to work through - before the model's internal parameters are updated.\n",
        "2. **Number of epochs** = a hyperparam of gradient descent - controls the number of complete passes - through the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiuSEtRMaTEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAS7NV_udNYl",
        "colab_type": "text"
      },
      "source": [
        "*Me* - **Hi! I'm Naga**<br>\n",
        "*SGD* - Hi I'm SGD - *Stochastic Gradient Descent* - used to train ML algos - notably ANN's used in DL. <br>\n",
        "\n",
        "*Me*: **What is your job?** <br>\n",
        "*SGD*: My job is - to find a set of internal model parameters - that perform well against some performance measure - logarithmic loss or mean squared error. <br>\n",
        "\n",
        "*Me*: **Ooh. What do you mean by optimization?** <br>\n",
        "*SGD*: As you know, I am an optimization algorithm - Optimization is a type of searching process, and you can think this search as learning.\n",
        "\n",
        "*Me*: **Then, Could you tell me about your name?**<br>\n",
        "*SGD*: Yes, sure. The optimization algorithm is called \"GRADIENT DESCENT\" - Gradient referes to calculation of an error gradient (or) slope of error - Descent referes to moving down along that slope towards some minimum level of error.\n",
        "\n",
        "*Me*: **You said about Iterative. What does it mean?** <br>\n",
        "*SGD*: I am Iterative algorithm - which means search process occurs over multiple discrete steps - each step (hopefully) slightly improves the model parameters.\n",
        "\n",
        "*Me*: **Could you elaborate your work in each step?**\n",
        "*SGD*: Yes - In each step, the model with the current set of internal parameters to make predictions on samples - I'll compare these predictions to the expected outcomes - calculate the error - and using the error - I'll update the internal model parameters.\n",
        "\n",
        "*Me*: **Any specific example?** <br>\n",
        "*SGD*: Backpropagation algorithm is used in your ANN's to update the model input parameters.\n",
        "\n",
        "*Me*: **Is that the only algorithm?** <br>\n",
        "*SGD*: No, there are different algorithms. Also, the update procedure is different for different algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceMjC6Odi1b8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4Svoph9jJwY",
        "colab_type": "text"
      },
      "source": [
        "**Sample**: A single row of data. <br>\n",
        "1. Contains inputs - fed into the algo\n",
        "2. Contains outputs - used to compare to the prediction and calculate an error\n",
        "3. Training data has many rows of data - means many samples.\n",
        "4. Also called an instance - an observation - an input vector - a feature vector\n",
        "\n",
        "**Batch**: A hyperparameter that defines the number of samples to work through - before updating internal model parameters\n",
        "1. A training dataset divided into 1 or more batches\n",
        "2. When all training samples - used to create 1 batch - the learning algorithm is called *Batch Gradient Descent* <center> Batch Size = Size of training set </center>\n",
        "3. When the batch size - is of 1 sample - the learning algorithm is called *Stochastic Gradient Descent* <center> Batch Size = 1</center>\n",
        "4. When the batch size - is more than 1 sample and less than size of training dataset - the learning algorithm is called *Mini-batch Gradient Descent* <center> 1 < Batch Size < Size of training set </center> Popular batch sizes include 32, 64, 128 samples\n",
        "\n",
        "**QnA**\n",
        "1. What if dataset does not divide evenly by batch size?\n",
        "A. Can remove few samples from dataset / Adjust the number of samples in the dataset does divide evenly by the batch size  \n",
        "*Reference* \n",
        " -  https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/\n",
        " - https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/\n",
        "\n",
        " **Epoch**: A hyperparameter that defines number of times - that learning algorithm will work through - the entire training dataset.\n",
        " 1. 1 epoch - each sample in dataset - gets an opportunity to update the internal model parameters.\n",
        " 2. An epoch is - comprised of 1 or more batches. \n",
        " <center>Example - An epoch which has 1 batch called Batch Gradient Descent Learning Algorithm </center>\n",
        " \n",
        "3. Can used to create line plots - epochs on X-axis, error or skill on Y-axis. \n",
        "4. These line plots are also called Learning curves\n",
        "5. These line plots helps to diagnose - whether the model has over learned, under learned or is suitably fit to training dataset.\n",
        "6. Generally, epochs are large in number. Until the error minimized, the learning algorithms will run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ckog5rW1i_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUCRKyEP1j_G",
        "colab_type": "text"
      },
      "source": [
        "**Batch vs Epoch**<br>\n",
        "Batch - Number of samples processed before the model is updated<br>\n",
        "Epoch - Number of complete passes through the training dataset <br>\n",
        "\n",
        "<br>\n",
        "Batch size must be >=1 or <=number of samples in dataset\n",
        "<br>\n",
        "number of epochs can be in b/w 1 to infinity.\n",
        "<br>\n",
        "<br>\n",
        "Batch / Epoch = <br>\n",
        "1. Integer values <br>\n",
        "2. Hyperparameters for learning algorithm but not internal model parameter <br>\n",
        "3. Must specify both for a learning algorithm <br>\n",
        "4. **No Rules to configure these parameters. Try different values until the model works best**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r3oAQhk3D54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Example - A dataset with 200 samples (rows of data), you choose a batch size of 5 and 1000 epochs.\n",
        "\n",
        "Batches per each epoch = (200 / 5) * 1 = 40\n",
        "Model weights (input parameters) will be updated after each batch of 5 samples.\n",
        "Total Batches for entire training process = (200 / 5) * 1000 = 40,000\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmtlPg5k3GzN",
        "colab_type": "text"
      },
      "source": [
        "Reference: https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnLZ_6y_3K_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}